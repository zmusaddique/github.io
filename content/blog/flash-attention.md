+++
date = '2025-03-28T22:22:25+05:30'
draft = true
title = 'Flash Attention - from first principles'
+++


Flash Attention is a concept to parallelize the attention mechanism leveraging the nature of hardware accelerators like GPU's. This blog aims at understanding nature of GPUs, understanding the math behind and implementing it in CUDA. Basic knowledge of [multi-head attention](https://arxiv.org/abs/1706.03762) and some high-school math would be great

